# -*- coding: utf-8 -*-
"""Kelompok7-Proyek UTS-Review Squid Game.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zhmu7T0ae3_2FtxXqC8zQPgri3FZCgDf

# **Proyek UTS - Review Netflix Series 'Squid Game'**

# Import File & Read .csv
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd 
import io

df = pd.read_csv(io.BytesIO(uploaded['Review Squid Game.csv']))
print(df)

"""# Scatter Text & Data Visualization

## **Instalasi**
"""

pip install scattertext

"""## **Import Library**"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import scattertext as st
import re, io
from pprint import pprint
import pandas as pd
import numpy as np
from scipy.stats import rankdata, hmean, norm
import spacy
import os, pkgutil, json, urllib
from urllib.request import urlopen
from IPython.display import IFrame
from IPython.core.display import display, HTML
from scattertext import CorpusFromPandas, produce_scattertext_explorer
display(HTML("<style>.container { width:98% !important; }</style>"))

"""## **Load Spacy**"""

import spacy
nlp = spacy.load('en')

"""## **Lihat Data**"""

df.head()

df.tail()

df[['Review']]

"""## **Parse Speech txt using Spacy**"""

df['parsed'] = df.Review.apply(nlp)

df.head()

"""## **Count of Positive, Negative, and Neutral Jenis Review**"""

print("Document Count")
print(df.groupby('Jenis Review')['Review'].count())
print("Word Count")
df.groupby('Jenis Review').apply(lambda x: x.Review.apply(lambda x: len(x.split())).sum())

"""## **Convert Dataframe into Scattertext Corpus**"""

corpus = st.CorpusFromParsedDocuments(df, category_col='Jenis Review', parsed_col='parsed').build()

print(corpus)

"""## **Visualize Chart**"""

html = produce_scattertext_explorer(corpus,category='Positive',category_name='Positive', not_category_name='Negative and Neutral',width_in_pixels=1000, minimum_term_frequency=5,metadata= df['Username'])
file_name = 'ScatterTextScale.html'
open(file_name, 'wb').write(html.encode('utf-8'))
IFrame(src=file_name, width = 1200, height=700)

html = st.produce_scattertext_explorer(corpus,
                                       category='Positive',
                                       category_name='Positive',
                                       not_category_name='Negative and Neutral',
                                       minimum_term_frequency=5,
                                       width_in_pixels=1000,
                                       transform=st.Scalers.log_scale_standardize)
file_name = 'ScattertextLog.html'
open(file_name, 'wb').write(html.encode('utf-8'))
IFrame(src=file_name, width = 1200, height=700)

html = produce_scattertext_explorer(corpus,
                                    category='Positive',
                                    category_name='Positive',
                                    not_category_name='Negative and Neutral',
                                    width_in_pixels=1000,
                                    minimum_term_frequency=5,
                                    transform=st.Scalers.percentile,
                                    metadata=df['Username'])
file_name = 'ScattertextRankData.html'
open(file_name, 'wb').write(html.encode('utf-8'))
IFrame(src=file_name, width = 1200, height=700)

"""# Text Processing NLTK & Word Cloud

## **Install & Import Library NLTK**
"""

!pip install nltk==3.5

import nltk
nltk.download('punkt')

"""## **Case Folding**"""

review = df[['Review']]

text_review = [str(i) for i in review.values]

paragraph = " ".join(text_review)
paragraph

lower_Review = paragraph.lower()
lower_Review

"""## **Tokenizing**"""

import string 
import re #regex library
from nltk.tokenize import word_tokenize

#remove angka
lower_example = re.sub(r"\d+", "", lower_Review)
lower_example

#remove punctuation
lower_example = lower_example.translate(str.maketrans("","",string.punctuation))
lower_example

#remove whitespace leading & trailing
lower_example = lower_example.strip()
lower_example

#remove multiple whitespace into single whitespace
lower_example = re.sub('\s+',' ',lower_example)
lower_example

tokens = nltk.tokenize.word_tokenize(lower_example)

print('Tokenizing Result : \n') 
tokens

"""## **Frekuensi Kata**"""

# import word_tokenize & FreqDist from NLTK
from nltk.tokenize import word_tokenize 
from nltk.probability import FreqDist

freq_tokens = nltk.FreqDist(tokens)

print('Frequency Tokens : \n') 
freq_tokens.most_common()

import pandas as pd

df_freq_tokens = pd.DataFrame.from_dict(freq_tokens, orient='index')
df_freq_tokens.columns = ['Frequency']
df_freq_tokens.index.name = 'Key'

df_freq_tokens.plot(kind='bar')

"""## **Filtering**"""

nltk.download('stopwords')

from nltk.corpus import stopwords

# tokenize text
freq_tokens

list_stopwords = set(stopwords.words('english'))

#remove stopword pada list token
tokens_without_stopword = [word for word in freq_tokens if not word in list_stopwords]


tokens_without_stopword

# import word_tokenize & FreqDist from NLTK
from nltk.tokenize import word_tokenize 
from nltk.probability import FreqDist

"""## **Stemming**"""

from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

stemmer = PorterStemmer()

# token without stopword
list_tokens = tokens_without_stopword

# stem
output   = [(token + " : " + stemmer.stem(token)) for token in list_tokens]

output

"""## **Text Visualization**"""

text=''
for t in output: 
  text += str(t) + ' '

import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS 
# stopwords is a collection of words that dont convey meaning. mostly pronouns such as he she etc.
wordcloud = WordCloud(width = 600, height = 300, random_state=1, background_color='white', collocations=False, stopwords = STOPWORDS).generate(text)
plt.figure(figsize=(40, 30))
# Display image
plt.imshow(wordcloud) 
# No axis 
plt.axis("off")
plt.show()